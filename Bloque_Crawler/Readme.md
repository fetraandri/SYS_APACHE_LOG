# BLOQUAGE_CRAWLERS


## 1.Recherchez dans le fichier journal Apache une liste d'agents utilisateurs.

$ cat /var/log/apache2/access.log 

## 2.Entrez dans le dossier apache2 avec le commande :

$ cd /var/log/apache2

## 3.Modifi√© √† l'aide un √©diteur (ex:vim ou nano) :

$ vim +x blockage.sh

Ou

$ nano blockage.sh

## 4. Entre le script:

$ !/bin/sh

L'utilisation principale de "awk" est de d√©composer chaque ligne d'un fichier en "champs" ou "colonnes" √† l'aide d'un s√©parateur pr√©d√©fini. √âtant donn√© que chaque ligne du fichier journal est bas√©e sur le format standard, nous pouvons faire beaucoup de choses assez facilement.

### 5.L'√©tape suivante consiste √† identifier les pages/fichiers qui g√©n√®rent les diff√©rents codes. La commande suivante r√©sumera les requ√™tes 404 ("Not Found") :

$ list all 404 requests
awk '($9 ~ /404/)' combined_log

$ summarise 404 requests

awk '($9 ~ /404/)' combined_log | awk '{print $9,$7}' | sort 

### 6.Modifiez les valeurs USER-AGENT pour refl√©ter vos besoins. 

$ RewriteCond %{HTTP_USER_AGENT} (gumgum-bot|postmanruntime|ag_dm_spider|scrapy|chimebot) [NC]

### Red√©marrez le service Apache 

$ service apache2 restart


#### 

F√©licitations! Vous avez appris √† configurer le serveur Apache pour refuser l‚Äôacc√®s aux utilisateurs et Crawlers.üëèüëè

 



